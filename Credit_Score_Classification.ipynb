{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff9b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17104\\1367173597.py:14: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(r'C:\\Users\\User\\Desktop\\Credit_Score_Classification\\train.csv')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statistics\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import data, add own location\n",
    "train_data = pd.read_csv(r'C:\\Users\\User\\Desktop\\Credit_Score_Classification\\train.csv')\n",
    "train_data = pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289cbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "# Drop Unimportant Columns\n",
    "data1 = train_data.drop(columns=['ID','Customer_ID','Month','Name','SSN','Monthly_Inhand_Salary',\n",
    "                           'Type_of_Loan','Num_of_Delayed_Payment','Credit_History_Age','Amount_invested_monthly'])\n",
    "# Drop weird value\n",
    "data1.drop(data1[data1['Monthly_Balance']=='__-333333333333333333333333333__'].index, inplace=True)\n",
    "\n",
    "\n",
    "# Deleting (-) and (_) in Column\n",
    "data1['Age'] = data1['Age'].str.replace('-','')\n",
    "data1['Age'] = data1['Age'].str.replace('_','')\n",
    "\n",
    "# Deleting (-) and (_) in Column\n",
    "data1['Annual_Income'] = data1['Annual_Income'].str.replace('-','')\n",
    "data1['Annual_Income'] = data1['Annual_Income'].str.replace('_','')\n",
    "\n",
    "# Deleting (-) and (_) in Column\n",
    "data1['Num_of_Loan'] = data1['Num_of_Loan'].str.replace('-','')\n",
    "data1['Num_of_Loan'] = data1['Num_of_Loan'].str.replace('_','')\n",
    "\n",
    "# Deleting (-) and (_) in Column\n",
    "data1['Changed_Credit_Limit'] = data1['Changed_Credit_Limit'].str.replace('-','0')\n",
    "data1['Changed_Credit_Limit'] = data1['Changed_Credit_Limit'].str.replace('_','0')\n",
    "\n",
    "# Deleting (-) and (_) in Column\n",
    "data1['Outstanding_Debt'] = data1['Outstanding_Debt'].str.replace('-','')\n",
    "data1['Outstanding_Debt'] = data1['Outstanding_Debt'].str.replace('_','')\n",
    "\n",
    "# Deleting (-) and (_) in Column\n",
    "data1['Monthly_Balance'] = data1['Monthly_Balance'].str.replace('-','')\n",
    "data1['Monthly_Balance'] = data1['Monthly_Balance'].str.replace('_','')\n",
    "\n",
    "# Replacing (!@9#%8) to (Unknown) in Column\n",
    "data1['Payment_Behaviour'] = data1['Payment_Behaviour'].str.replace('!@9#%8','Unknown')\n",
    "\n",
    "\n",
    "\n",
    "data1['Age'] = data1['Age'].astype(int)\n",
    "data1['Annual_Income'] = data1['Annual_Income'].astype(float)\n",
    "data1['Num_of_Loan'] = data1['Num_of_Loan'].astype(int)\n",
    "data1['Changed_Credit_Limit'] = data1['Changed_Credit_Limit'].astype(float)\n",
    "data1['Outstanding_Debt'] = data1['Outstanding_Debt'].astype(float)\n",
    "data1['Monthly_Balance'] = data1['Monthly_Balance'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data1['Payment_of_Min_Amount'] = data1['Payment_of_Min_Amount'].str.replace('NM','0')\n",
    "data1['Payment_of_Min_Amount'] = data1['Payment_of_Min_Amount'].str.replace('No','1')\n",
    "data1['Payment_of_Min_Amount'] = data1['Payment_of_Min_Amount'].str.replace('Yes','2')\n",
    "data1['Payment_of_Min_Amount']=data1['Payment_of_Min_Amount'].astype(int)\n",
    "\n",
    "\n",
    "data1['Age'].where((data1['Age']<=80),np.nan, inplace=True)\n",
    "data1['Age'] = data1['Age'].fillna(method='ffill')\n",
    "\n",
    "\n",
    "# Handling missing values with mean\n",
    "data1['Monthly_Balance'] = data1['Monthly_Balance'].fillna(data1['Monthly_Balance'].mean())\n",
    "data1['Num_Credit_Inquiries'] = data1['Num_Credit_Inquiries'].fillna(data1['Num_Credit_Inquiries'].mean())\n",
    "\n",
    "# Handling missing values with ffill\n",
    "data1['Occupation'] = data1['Occupation'].replace('_______', method='ffill')\n",
    "data1.head()\n",
    "\n",
    "# Handling missing values with bfill\n",
    "data1['Credit_Mix'] = data1['Credit_Mix'].replace('_', method='bfill')\n",
    "data1.head()\n",
    "\n",
    "\n",
    "data1['Credit_Mix'] = data1['Credit_Mix'].str.replace('Bad','0')\n",
    "data1['Credit_Mix'] = data1['Credit_Mix'].str.replace('Standard','1')\n",
    "data1['Credit_Mix'] = data1['Credit_Mix'].str.replace('Good','2')\n",
    "data1['Credit_Mix'] = data1['Credit_Mix'].astype(int)\n",
    "data1.groupby(['Credit_Mix']).count()[['Occupation']]\n",
    "\n",
    "\n",
    "\n",
    "data1['Payment_Behaviour'] = data1['Payment_Behaviour'].str.replace('Unknown','0')\n",
    "data1['Payment_Behaviour'] = data1['Payment_Behaviour'].str.replace('Low_spent_Small_value_payments','1')\n",
    "data1['Payment_Behaviour'] = data1['Payment_Behaviour'].str.replace('Low_spent_Medium_value_payments','2')\n",
    "data1['Payment_Behaviour'] = data1['Payment_Behaviour'].str.replace('Low_spent_Large_value_payments','3')\n",
    "data1['Payment_Behaviour'] = data1['Payment_Behaviour'].str.replace('High_spent_Small_value_payments','4')\n",
    "data1['Payment_Behaviour'] = data1['Payment_Behaviour'].str.replace('High_spent_Medium_value_payments','5')\n",
    "data1['Payment_Behaviour'] = data1['Payment_Behaviour'].str.replace('High_spent_Large_value_payments','6')\n",
    "data1['Payment_Behaviour'] = data1['Payment_Behaviour'].astype(int)\n",
    "data1.groupby(['Payment_Behaviour']).count()[['Occupation']]\n",
    "\n",
    "data1['Credit_Score'] = data1['Credit_Score'].str.replace('Poor','0')\n",
    "data1['Credit_Score'] = data1['Credit_Score'].str.replace('Standard','1')\n",
    "data1['Credit_Score'] = data1['Credit_Score'].str.replace('Good','2')\n",
    "data1['Credit_Score'] = data1['Credit_Score'].astype(int)\n",
    "data1.groupby(['Credit_Score']).count()[['Occupation']]\n",
    "\n",
    "\n",
    "\n",
    "data1 = pd.get_dummies(data1, columns=['Occupation'], prefix=['Occ_'])\n",
    "\n",
    "data2=data1.copy()\n",
    "\n",
    "# Capping the outliers with the extreme values in the data distribution range: Winsorization\n",
    "def treat_outliers(col, df):\n",
    "    lower_limit, upper_limit = df[col].quantile([0.25,0.75])\n",
    "    IQR = upper_limit - lower_limit\n",
    "    lower_whisker = lower_limit - 1.5 * IQR\n",
    "    upper_whisker = upper_limit + 1.5 * IQR   \n",
    "    return np.where(df[col]>upper_whisker,upper_whisker,np.where(df[col]<lower_whisker,lower_whisker,df[col]))\n",
    "\n",
    "outlier_cols = ['Total_EMI_per_month','Credit_Utilization_Ratio','Num_Credit_Inquiries','Interest_Rate','Num_Credit_Card']\n",
    "\n",
    "for col in outlier_cols:\n",
    "    data1[col] = treat_outliers(col, data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25bbab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monthly_Balance</th>\n",
       "      <td>99991.00</td>\n",
       "      <td>402.16</td>\n",
       "      <td>210.56</td>\n",
       "      <td>0.01</td>\n",
       "      <td>271.83</td>\n",
       "      <td>341.40</td>\n",
       "      <td>463.50</td>\n",
       "      <td>1602.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>99991.00</td>\n",
       "      <td>33.32</td>\n",
       "      <td>10.77</td>\n",
       "      <td>14.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>56.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_Utilization_Ratio</th>\n",
       "      <td>99991.00</td>\n",
       "      <td>32.28</td>\n",
       "      <td>5.12</td>\n",
       "      <td>20.00</td>\n",
       "      <td>28.05</td>\n",
       "      <td>32.31</td>\n",
       "      <td>36.50</td>\n",
       "      <td>49.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annual_Income</th>\n",
       "      <td>99991.00</td>\n",
       "      <td>176426.79</td>\n",
       "      <td>1429681.87</td>\n",
       "      <td>7005.93</td>\n",
       "      <td>19456.50</td>\n",
       "      <td>37578.61</td>\n",
       "      <td>72790.92</td>\n",
       "      <td>24198062.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_of_Loan</th>\n",
       "      <td>99991.00</td>\n",
       "      <td>10.76</td>\n",
       "      <td>61.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1496.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            count      mean        std     min      25%  \\\n",
       "Monthly_Balance          99991.00    402.16     210.56    0.01   271.83   \n",
       "Age                      99991.00     33.32      10.77   14.00    24.00   \n",
       "Credit_Utilization_Ratio 99991.00     32.28       5.12   20.00    28.05   \n",
       "Annual_Income            99991.00 176426.79 1429681.87 7005.93 19456.50   \n",
       "Num_of_Loan              99991.00     10.76      61.79    0.00     2.00   \n",
       "\n",
       "                              50%      75%         max  \n",
       "Monthly_Balance            341.40   463.50     1602.04  \n",
       "Age                         33.00    42.00       56.00  \n",
       "Credit_Utilization_Ratio    32.31    36.50       49.16  \n",
       "Annual_Income            37578.61 72790.92 24198062.00  \n",
       "Num_of_Loan                  3.00     6.00     1496.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#overview over selected variables\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "data1[[\"Monthly_Balance\",  \"Age\", \"Credit_Utilization_Ratio\", \"Annual_Income\", \"Num_of_Loan\"]].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e963ca61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99991, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN,RandomOverSampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#balance data for better training for underrepresented classes\n",
    "def split_and_balance(df,target='Credit_Score',test_size=0.3,random_state=0,\n",
    "                      augmentation=RandomOverSampler(random_state=16),remove_loan_type=False):\n",
    "\n",
    "    if remove_loan_type:\n",
    "        df=df.loc[:,:'Credit_Score']\n",
    "    test=pd.DataFrame()\n",
    "    nuniq_labels=df[target].nunique()\n",
    "\n",
    "    for i,l in enumerate(df[target].unique()):\n",
    "        if i ==0:\n",
    "            test=df.query(f'Credit_Score=={l}').sample(n=int((df.shape[0]*test_size)//nuniq_labels)\n",
    "                                                       ,random_state=random_state)\n",
    "        else:\n",
    "            test=pd.concat([test,df.query(f'Credit_Score=={l}')\n",
    "                           .sample(n=int((df.shape[0]*test_size)//nuniq_labels),\n",
    "                                   random_state=random_state)])\n",
    "    train=df.drop(test.index)\n",
    "\n",
    "    if augmentation:\n",
    "        xtrain,ytrain=augmentation.fit_resample(train.drop(target,axis=1),\n",
    "                                                train[target])\n",
    "\n",
    "    else:\n",
    "        xtrain,ytrain=train.drop(target,axis=1),train[target]\n",
    "\n",
    "    xtest,ytest=test.drop(target,axis=1),test[target]\n",
    "\n",
    "    return xtrain,xtest,ytrain,ytest\n",
    "\n",
    "\n",
    "normal_columns=['Age', 'Annual_Income', 'Num_Bank_Accounts', 'Num_Credit_Card',\n",
    "       'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',\n",
    "       'Changed_Credit_Limit', 'Num_Credit_Inquiries',\n",
    "       'Outstanding_Debt', 'Credit_Utilization_Ratio',\n",
    "       'Total_EMI_per_month', 'Monthly_Balance']\n",
    "\n",
    "#normalize columns, as model doesn't converge when not done\n",
    "for cl in normal_columns:\n",
    "    data1[cl] =(( data1[cl] - data1[cl].mean() ) / data1[cl].std())\n",
    "\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33f915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split\n",
    "X_train, X_test, y_train, y_test=split_and_balance(df=data1,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da85dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchbnn as bnn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5485c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert train, test data to tensor\n",
    "\n",
    "x, y = torch.from_numpy(X_train.to_numpy()).float(), torch.from_numpy(y_train.to_numpy()).long()\n",
    "\n",
    "x_t, y_t = torch.from_numpy(X_test.to_numpy()).float(), torch.from_numpy(y_test.to_numpy()).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4d3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and train BNN-model\n",
    "model = nn.Sequential(\n",
    "    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=31, out_features=200),\n",
    "    nn.ReLU(),\n",
    "    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=200, out_features=3), \n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "kl_weight = 1.0\n",
    "\n",
    "\n",
    "n_epochs = 100  # number of epochs to run\n",
    "batch_size = 512  # size of each batch\n",
    "batches_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "\n",
    "\n",
    "loss_sum=0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(batches_per_epoch):\n",
    "        start = i * batch_size\n",
    "        # take a batch\n",
    "        Xbatch = x[start:start+batch_size]\n",
    "        ybatch = y[start:start+batch_size]\n",
    "        # forward pass\n",
    "        y_pred = model(Xbatch)\n",
    "        ce = ce_loss(y_pred, ybatch)\n",
    "        kl = kl_loss(model)\n",
    "        loss = ce + kl_weight*kl\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c71421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#für plot\n",
    "\n",
    "from scipy import stats\n",
    "import collections\n",
    "\n",
    "\n",
    "bnn_models_result_raw=[]\n",
    "bnn_models_result_raw2=[]\n",
    "for k in range(100):\n",
    "    bnn_models_result_raw.append(np.exp(model(x_t).data))\n",
    "    bnn_models_result_raw2.append(np.array(np.exp(model(x_t).data)))\n",
    "\n",
    "\n",
    "\n",
    "#append index with the highest softmax output and the data\n",
    "bnn_models_result_max_index=[]\n",
    "bnn_models_result_rawda=[]\n",
    "for tens in bnn_models_result_raw:\n",
    "    bnn_models_result_max_index.append(np.array(torch.max(tens, 1)[1]))\n",
    "    max_indx=np.array(torch.max(tens, 1)[1])\n",
    "    bnn_models_result_rawda.append(np.array(tens))\n",
    "\n",
    "bnn_models_result_max_index=np.array(bnn_models_result_max_index).T\n",
    "bnn_models_result_rawda=np.array(bnn_models_result_rawda).T\n",
    "\n",
    "\n",
    "#list of most common indexes\n",
    "bnn_mfreq = [collections.Counter(i).most_common()[0][0] for i in bnn_models_result_max_index]\n",
    "\n",
    "coll_vals_ges=[]\n",
    "\n",
    "#collect uncertainty threshold for every prediction\n",
    "for i in range (0,len(bnn_mfreq)):\n",
    "    coll_vals=[]\n",
    "    for runs in range (0,100):\n",
    "        coll_vals.append(bnn_models_result_raw2[runs][i][bnn_mfreq[i]])\n",
    "    coll_vals_ges.append(np.median(coll_vals))\n",
    "\n",
    "\n",
    "ytlist=y_t.tolist()\n",
    "rejection_df=pd.DataFrame(data=zip(bnn_mfreq, coll_vals_ges, ytlist), columns=[\"mfreq\", \"med_prob\", \"y_true\"])\n",
    "\n",
    "\n",
    "X_test_thresh_org=X_test.reset_index(drop=True)\n",
    "X_test_thresh=X_test.reset_index(drop=True)\n",
    "X_test_thresholds=[]\n",
    "y_test_thresholds=[]\n",
    "y_pred_thresholds=[]\n",
    "accuracies=[]\n",
    "percent_classified=[]\n",
    "label_distri=[]\n",
    "label_distri2=[]\n",
    "\n",
    "total_org=len(rejection_df['mfreq'])\n",
    "rejection_df_org=rejection_df\n",
    "\n",
    "#list of defined thresholds\n",
    "thresholds=[0, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for threshold in thresholds:\n",
    "\n",
    "        #reject items with thresholds smaller than the current threshold\n",
    "        for item in rejection_df.iterrows():\n",
    "            if(item[1][1]<threshold):\n",
    "                rejection_df.drop(index=item[0], inplace=True)\n",
    "                X_test_thresh.drop(index=item[0], inplace=True)\n",
    "\n",
    "        y_pred_thresholds.append(rejection_df[\"mfreq\"])\n",
    "        rejection_df=rejection_df.reset_index(drop=True)\n",
    "        X_test_thresh=X_test_thresh.reset_index(drop=True)\n",
    "        X_test_thresholds.append(X_test_thresh)\n",
    "        y_test_thresholds.append(rejection_df['y_true'])\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        total = len(rejection_df['mfreq'])\n",
    "\n",
    "        #count correct classifications\n",
    "        correct=0\n",
    "        for i in range(0,len(rejection_df['mfreq'])):\n",
    "                       if(rejection_df['mfreq'][i]==(rejection_df['y_true'][i])):\n",
    "                            correct=correct+1\n",
    "\n",
    "        #collects accuracies and classified percent for each threshold\n",
    "        accuracies.append(100 * float(correct) / total)\n",
    "        percent_classified.append(100*(total/total_org))\n",
    "        rejection_df=rejection_df_org\n",
    "        X_test_thresh=X_test_thresh_org\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15667f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.6553\n"
     ]
    }
   ],
   "source": [
    "#Gaussian Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "\n",
    "# fit the model\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a06500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple NN classifier\n",
    "\n",
    "model2 = nn.Sequential(\n",
    "    nn.Linear(in_features=31, out_features=100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=100, out_features=3), \n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.0001)\n",
    "\n",
    "x_t, y_t = torch.from_numpy(X_test.to_numpy()).float(), torch.from_numpy(y_test.to_numpy()).long()\n",
    "\n",
    "x, y = torch.from_numpy(X_train.to_numpy()).float(), torch.from_numpy(y_train.to_numpy()).long()\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "n_epochs = 100  \n",
    "batch_size = 512  \n",
    "batches_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_sum=0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(batches_per_epoch):\n",
    "        start = i * batch_size\n",
    "        # take a batch\n",
    "        Xbatch = x[start:start+batch_size]\n",
    "        ybatch = y[start:start+batch_size]\n",
    "        # forward pass\n",
    "        y_pred = model2(Xbatch)\n",
    "        ce = ce_loss(y_pred, ybatch)\n",
    "        loss = ce\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        loss_sum=loss_sum+loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ec2c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.6776\n"
     ]
    }
   ],
   "source": [
    "y_pred= model2(x_t)\n",
    "y_pred=np.array(torch.max(y_pred, 1)[1])\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0511c80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.50</th>\n",
       "      <th>0.60</th>\n",
       "      <th>0.70</th>\n",
       "      <th>0.80</th>\n",
       "      <th>0.90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BNN Acc.</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BNN F1</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNN Acc.</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNN F1</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNB Acc.</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNB F1</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share accepted</th>\n",
       "      <td>100.00</td>\n",
       "      <td>69.96</td>\n",
       "      <td>53.82</td>\n",
       "      <td>33.48</td>\n",
       "      <td>12.43</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0.00  0.50  0.60  0.70  0.80  0.90\n",
       "BNN Acc.         0.64  0.70  0.74  0.78  0.80  0.84\n",
       "BNN F1           0.62  0.56  0.55  0.57  0.59  0.59\n",
       "DNN Acc.         0.68  0.71  0.74  0.78  0.80  0.84\n",
       "DNN F1           0.66  0.58  0.55  0.57  0.59  0.59\n",
       "GNB Acc.         0.66  0.70  0.74  0.78  0.80  0.84\n",
       "GNB F1           0.64  0.58  0.55  0.57  0.59  0.59\n",
       "share accepted 100.00 69.96 53.82 33.48 12.43  1.40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate models for different confidence thresholds\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "BNN_acc=[]\n",
    "BNN_f1=[]\n",
    "DNN_acc=[]\n",
    "DNN_f1=[]\n",
    "GNB_acc=[]\n",
    "GNB_f1=[]\n",
    "for i in range(len(X_test_thresholds)):\n",
    "    x_tt, y_tt = torch.from_numpy(X_test_thresholds[i].to_numpy()).float(), torch.from_numpy(y_test_thresholds[i].to_numpy()).long()\n",
    "    y_pred= model2(x_tt)\n",
    "    y_pred=np.array(torch.max(y_pred, 1)[1])\n",
    "    DNN_acc.append(accuracy_score(y_test_thresholds[i], y_pred))\n",
    "    DNN_f1.append(f1_score(y_test_thresholds[i], y_pred, average=\"macro\"))\n",
    "    y_pred=None\n",
    "    \n",
    "    \n",
    "    BNN_acc.append(accuracy_score(y_test_thresholds[i], y_pred_thresholds[i]))\n",
    "    BNN_f1.append(f1_score(y_test_thresholds[i], y_pred_thresholds[i], average=\"macro\"))\n",
    "    \n",
    "    \n",
    "    y_pred = gnb.predict(X_test_thresholds[i])\n",
    "    GNB_acc.append(accuracy_score(y_test_thresholds[i], y_pred))\n",
    "    GNB_f1.append(f1_score(y_test_thresholds[i], y_pred, average=\"macro\"))\n",
    "\n",
    "rejection_df=pd.DataFrame(data=zip(BNN_acc, BNN_f1,DNN_acc, DNN_f1,GNB_acc, GNB_f1, percent_classified), index=thresholds, \n",
    "                          columns=[\"BNN Acc.\", \"BNN F1\",\"DNN Acc.\", \"DNN F1\",\"GNB Acc.\", \"GNB F1\", \"share accepted\"])\n",
    "\n",
    "\n",
    "\n",
    "rejection_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a7750b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 9999, 1: 9999, 0: 9999})\n",
      "Counter({0: 8091, 2: 8039, 1: 4856})\n",
      "Counter({0: 6540, 2: 6387, 1: 3218})\n",
      "Counter({0: 4432, 2: 3747, 1: 1865})\n",
      "Counter({0: 2205, 2: 837, 1: 686})\n",
      "Counter({0: 341, 1: 66, 2: 13})\n"
     ]
    }
   ],
   "source": [
    "for i in y_test_thresholds:\n",
    "    print(collections.Counter(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b673a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use unmodified data as basis for outliers\n",
    "\n",
    "x = data2.drop('Credit_Score', axis=1) # Independet Variable\n",
    "x = torch.from_numpy(x.to_numpy()).float()\n",
    "\n",
    "\n",
    "max_input_np = np.zeros((8, x.shape[1]))\n",
    "\n",
    "#make outliers\n",
    "#(indexes 0,1: small, common values; 2,3:high, uncommon values; 4,5: median, mean values, 6,7: zero(nonsense values) )\n",
    "#low values\n",
    "for i in range(x.shape[1]):\n",
    "    max_input_np[0, i] = torch.amin(x[:, i]) * 1\n",
    "\n",
    "for i in range(x.shape[1]):\n",
    "    max_input_np[1, i] = torch.amin(x[:, i]) * 0.1\n",
    "\n",
    "    \n",
    "    \n",
    "for i in [6,7,10]:\n",
    "    max_input_np[0, i] = torch.amax(x[:, i]) * 1    \n",
    "for i in [6,7,10]:\n",
    "    max_input_np[1, i] = torch.amax(x[:, i]) * 10\n",
    "\n",
    "    \n",
    "#high values   \n",
    "for i in range(x.shape[1]):\n",
    "    max_input_np[2, i] = torch.amax(x[:, i]) * 1    \n",
    "for i in range(x.shape[1]):\n",
    "    max_input_np[3, i] = torch.amax(x[:, i]) * 10\n",
    "    \n",
    "for i in [6,7,10]:\n",
    "    max_input_np[2, i] = torch.amin(x[:, i]) * 1    \n",
    "for i in [6,7,10]:\n",
    "    max_input_np[3, i] = torch.amin(x[:, i]) * 0.1\n",
    "\n",
    "#average values\n",
    "for i in range(x.shape[1]):\n",
    "    max_input_np[4, i] = torch.median(x[:, i])      \n",
    "for i in range(x.shape[1]):\n",
    "    max_input_np[5, i] = torch.mean(x[:, i])\n",
    "\n",
    "#nonsense values \n",
    "for i in range(x.shape[1]):\n",
    "    max_input_np[6, i] = 0\n",
    "    \n",
    "for i in range(x.shape[1]):\n",
    "    max_input_np[7, i] = torch.median(x[:, i])\n",
    "\n",
    "    \n",
    "#manually set categorical attributes    \n",
    "for i in [0,1,4,5]:\n",
    "    max_input_np[i,0] = torch.median(x[:, 0])\n",
    "    max_input_np[i,9] = 1\n",
    "    max_input_np[i,12] = 2\n",
    "    max_input_np[i,14] = 1\n",
    "    for b in range(16,31):\n",
    "        max_input_np[i,b] = 0\n",
    "    max_input_np[i,23] = 1\n",
    "\n",
    "for i in [2,3]:\n",
    "    max_input_np[i,0] = torch.median(x[:, 0])\n",
    "    max_input_np[i,9] = 0\n",
    "    max_input_np[i,12] = 0\n",
    "    max_input_np[i,14] = 0\n",
    "    for b in range(16,31):\n",
    "        max_input_np[i,b] = 0\n",
    "    max_input_np[i,30] = 1\n",
    "        \n",
    "for i in [7]:\n",
    "    max_input_np[i,9] = 1\n",
    "    max_input_np[i,12] = 2\n",
    "    max_input_np[i,14] = 1\n",
    "    for b in range(16,31):\n",
    "        max_input_np[i,b] = 1\n",
    "        \n",
    "        \n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\n",
    "#normalize manufactured data\n",
    "max_i_df=pd.DataFrame(max_input_np, columns=data2.drop('Credit_Score', axis=1).columns)\n",
    "for cl in normal_columns:\n",
    "    max_i_df[cl] =(( max_i_df[cl] - data2[cl].mean() ) / data2[cl].std())\n",
    "\n",
    "    \n",
    "x_ttt = torch.from_numpy(max_i_df.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5840cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>min*10</th>\n",
       "      <th>max</th>\n",
       "      <th>max*10</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>zeros</th>\n",
       "      <th>median+nons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction</th>\n",
       "      <td>'Poor'</td>\n",
       "      <td>'Poor'</td>\n",
       "      <td>'Poor'</td>\n",
       "      <td>'Poor'</td>\n",
       "      <td>'Standard'</td>\n",
       "      <td>'Poor'</td>\n",
       "      <td>'Standard'</td>\n",
       "      <td>'Standard'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median probability</th>\n",
       "      <td>0.59</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       min  min*10     max  max*10      median    mean  \\\n",
       "prediction          'Poor'  'Poor'  'Poor'  'Poor'  'Standard'  'Poor'   \n",
       "median probability    0.59    1.00    1.00    1.00        0.40    0.42   \n",
       "\n",
       "                         zeros median+nons  \n",
       "prediction          'Standard'  'Standard'  \n",
       "median probability        0.32        0.39  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict outliers\n",
    "\n",
    "from scipy import stats\n",
    "import collections\n",
    "\n",
    "#predict every entry 100x\n",
    "np.exp(model(x_ttt).data)\n",
    "bnn_models_result_raw=[]\n",
    "bnn_models_result_raw2=[]\n",
    "for k in range(100):\n",
    "    bnn_models_result_raw.append(np.exp(model(x_ttt).data))\n",
    "    bnn_models_result_raw2.append(np.array(np.exp(model(x_ttt).data)))\n",
    "\n",
    "#collect indexes with highest softmax\n",
    "bnn_models_result_max_index=[]\n",
    "bnn_models_result_rawda=[]\n",
    "\n",
    "\n",
    "\n",
    "for tens in bnn_models_result_raw:\n",
    "    bnn_models_result_max_index.append(np.array(torch.max(tens, 1)[1]))\n",
    "    max_indx=np.array(torch.max(tens, 1)[1])\n",
    "    bnn_models_result_rawda.append(np.array(tens))\n",
    "\n",
    "bnn_models_result_max_index=np.array(bnn_models_result_max_index).T\n",
    "bnn_models_result_rawda=np.array(bnn_models_result_rawda).T\n",
    "\n",
    "\n",
    "#collect most common indexes for every entry(=classification prediction)\n",
    "bnn_mfreq = [collections.Counter(i).most_common()[0][0] for i in bnn_models_result_max_index]\n",
    "\n",
    "\n",
    "#collect median softmax values\n",
    "coll_vals_ges=[]\n",
    "\n",
    "for i in range (0,len(bnn_mfreq)):\n",
    "    coll_vals=[]\n",
    "    for runs in range (0,100):\n",
    "        coll_vals.append(bnn_models_result_raw2[runs][i][bnn_mfreq[i]])\n",
    "    coll_vals_ges.append(np.median(coll_vals))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#create rejection_df for outlier values\n",
    "ytlist=y_t.tolist()\n",
    "rejection_df=pd.DataFrame(data=zip(bnn_mfreq, coll_vals_ges), columns=[\"mfreq\", \"med_prob\"], \n",
    "                          index=[\"min\", \"min*10\",\"max\", \"max*10\",\"median\", \"mean\",\"zeros\", \"median+nons\"])\n",
    "\n",
    "\n",
    "total_org=len(rejection_df['mfreq'])\n",
    "rejection_df_org=rejection_df\n",
    "\n",
    "\n",
    "        \n",
    "rejection_df = rejection_df.rename(columns={\"mfreq\":\"prediction\", \"med_prob\" :\"median probability\"})\n",
    "rejection_df['prediction'].replace(2, \"'Good'\", inplace=True)\n",
    "rejection_df['prediction'].replace(1, \"'Standard'\", inplace=True)\n",
    "rejection_df['prediction'].replace(0, \"'Poor'\", inplace=True)\n",
    "\n",
    "        \n",
    "               \n",
    "    \n",
    "rejection_df.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
